# Here you can define all your data sets by using simple YAML syntax.
#
# Documentation for this file format can be found in "The Data Catalog"
# Link: https://kedro.readthedocs.io/en/stable/05_data/01_data_catalog.html

# Dataset
allocine_train:
  type: pandas.CSVDataSet
  filepath: data/01_raw/allocine_train.csv
  fs_args:
    open_args_load:
      encoding: 'utf_8'

allocine_valid:
  type: pandas.CSVDataSet
  filepath: data/01_raw/allocine_valid.csv
  fs_args:
    open_args_load:
      encoding: 'utf_8'

allocine_test:
  type: pandas.CSVDataSet
  filepath: data/01_raw/allocine_test.csv
  fs_args:
    open_args_load:
      encoding: 'utf_8'

## CNN Character
#cnn_char_model:
#  type: deep_nlp.extras.datasets.local_model.TorchLocalModel
#  filepath: data/06_models/cnn_char_classifier/cnn_char_model/cnn_char_model.pth.tar
#  model: "CNNCharClassifier"

cnn_char_model:
  type: kedro_mlflow.io.artifacts.MlflowArtifactDataSet
  data_set:
    type: pickle.PickleDataSet
    filepath: data/06_models/cnn_char_classifier/cnn_char_model/cnn_char_model.pt

# Logistic BOW

vocabulary_logistic_bow:
  type: kedro.extras.datasets.pickle.pickle_dataset.PickleDataSet
  filepath: data/05_model_input/logistic_bow/vocabulary.pkl

train_data_logistic_bow:
  type: kedro.extras.datasets.pickle.pickle_dataset.PickleDataSet
  filepath: data/05_model_input/logistic_bow/train_data.pkl

train_y_logistic_bow:
  type: kedro.extras.datasets.pickle.pickle_dataset.PickleDataSet
  filepath: data/05_model_input/logistic_bow/train_y.pkl

valid_data_logistic_bow:
  type: kedro.extras.datasets.pickle.pickle_dataset.PickleDataSet
  filepath: data/05_model_input/logistic_bow/valid_data.pkl

valid_y_logistic_bow:
  type: kedro.extras.datasets.pickle.pickle_dataset.PickleDataSet
  filepath: data/05_model_input/logistic_bow/valid_y.pkl

test_data_logistic_bow:
  type: kedro.extras.datasets.pickle.pickle_dataset.PickleDataSet
  filepath: data/05_model_input/logistic_bow/test_data.pkl

test_y_logistic_bow:
  type: kedro.extras.datasets.pickle.pickle_dataset.PickleDataSet
  filepath: data/05_model_input/logistic_bow/test_y.pkl

logistic_bow_model_train:
  type: kedro_mlflow.io.artifacts.MlflowArtifactDataSet
  data_set:
    type: pickle.PickleDataSet
    filepath: data/06_models/logisticclassifier/logistic_bow_model/logistic_bow.pkl

logistic_bow_scaler:
  type: kedro.extras.datasets.pickle.pickle_dataset.PickleDataSet
  filepath: data/06_models/logisticclassifier/logistic_bow_model/scaler.pkl


# CNN + Embed

word2vec:
  type: pandas.CSVDataSet
  filepath: data/01_raw/word2vec.csv
  fs_args:
    open_args_load:
      encoding: 'latin-1'

nlp:
  type: pickle.PickleDataSet
  filepath: data/01_raw/nlp.pickle
  backend: pickle

save_embedding:
  type: pandas.CSVDataSet
  filepath: data/02_intermediate/word2vec_from_torch.csv
  fs_args:
    open_args_save:
      encoding: 'latin-1'


# BERT + CNN

camembert_tokenizer:
  type: kedro.extras.datasets.pickle.pickle_dataset.PickleDataSet
  filepath: data/06_models/bert_cnn/camembert/initial/camembert_tokenizer.pkl

train_bert_tensor:
  type: kedro.extras.datasets.pickle.pickle_dataset.PickleDataSet
  filepath: data/05_model_input/bert_cnn/train_bert_tensor.pkl

valid_bert_tensor:
  type: kedro.extras.datasets.pickle.pickle_dataset.PickleDataSet
  filepath: data/05_model_input/bert_cnn/valid_bert_tensor.pkl

test_bert_tensor:
  type: kedro.extras.datasets.pickle.pickle_dataset.PickleDataSet
  filepath: data/05_model_input/bert_cnn/test_bert_tensor.pkl

word2vec_bilstm:
  type: kedro.extras.datasets.pickle.pickle_dataset.PickleDataSet
  filepath: data/06_models/word2vec/word2vec.pkl